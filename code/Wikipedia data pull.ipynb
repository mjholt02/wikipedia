{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08db2ac8",
   "metadata": {},
   "source": [
    "Wikipedia rest API located at: https://wikimedia.org/api/rest_v1/#/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca584804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5213f",
   "metadata": {},
   "source": [
    "## Getting pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97301967",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{w}/{x}/{y}/daily/20100101/20220101\"\n",
    "l1 = ['desktop','mobile-app','mobile-web']\n",
    "l2 = ['user','spider','automated']\n",
    "l3 = ['commons.wikimedia.org','en.wikipedia.org']\n",
    "\n",
    "combined = [(f,s,v) for f in l1 for s in l2 for v in l3]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.DataFrame(pd.read_json(base_html.format(x=j[0],y=j[1], w = j[2]))['items'].values.tolist())\n",
    "    if i == 0:\n",
    "        pageviews = d\n",
    "    else: \n",
    "        pageviews = pd.concat([pageviews,d])\n",
    "pageviews.to_csv(\"./data/pageviews_aggregate.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afcff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{w}/all-access/{cyear}/{cmo}/{cday}\"\n",
    "\n",
    "yearList = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\"]\n",
    "monthList = [f\"0{i}\" for i in range(1,10)] + [i for i in range(10,13)]\n",
    "dayList = [f\"0{i}\" for i in range(1,10)] + [i for i in range(10,32)]\n",
    "l3 = ['commons.wikimedia.org','en.wikipedia.org']\n",
    "\n",
    "combined = [(a,b,c,d) for a in yearList for b in monthList for c in dayList for d in l3]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    time.sleep(0.25)\n",
    "    try:\n",
    "        d = pd.json_normalize(pd.read_json(base_html.format(cyear = j[0],cmo = j[1],cday=j[2],w = j[3]))['items'], \n",
    "                                            record_path =['articles'], \n",
    "                                            meta =['project','access','year','month','day'])\n",
    "        if i == 0:\n",
    "            top_pages = d\n",
    "        else: \n",
    "            top_pages = pd.concat([top_pages,d])\n",
    "            \n",
    "    except: xxx=5\n",
    "            \n",
    "top_pages.reset_index().to_csv(\"./data/pageviews_top.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fe3be",
   "metadata": {},
   "source": [
    "## Unique devices count by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf076e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/unique-devices/en.wikipedia.org/{x}/daily/20150101/20220101\"\n",
    "d = pd.DataFrame(pd.read_json(base_html.format(x='desktop-site'))['items'].values.tolist())\n",
    "ma = pd.DataFrame(pd.read_json(base_html.format(x='mobile-site'))['items'].values.tolist())\n",
    "\n",
    "devices = pd.concat([d,ma])\n",
    "\n",
    "devices.reset_index().to_csv(\"./data/unique_devices.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448333d3",
   "metadata": {},
   "source": [
    "# Edited pages data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d63fe3",
   "metadata": {},
   "source": [
    "## New pages by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3304bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/edited-pages/new/en.wikipedia/{x}/{y}/daily/20150101/20220101\"\n",
    "             \n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "\n",
    "combined = [(f,s) for f in l1 for s in l2]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','granularity'])\n",
    "    if i == 0:\n",
    "        new_pages = d\n",
    "    else: \n",
    "        new_pages = pd.concat([new_pages,d])\n",
    "new_pages.reset_index().to_csv(\"./data/edited_pages_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669b342",
   "metadata": {},
   "source": [
    "## Edited pages counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac22af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/edited-pages/aggregate/en.wikipedia/{x}/{y}/all-activity-levels/daily/20150101/20220101\"\n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "\n",
    "combined = [(f,s) for f in l1 for s in l2]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','activity-level','granularity'])\n",
    "    if i == 0:\n",
    "        edited_pages = d\n",
    "    else: \n",
    "        edited_pages = pd.concat([edited_pages,d])\n",
    "edited_pages.reset_index().to_csv(\"./data/edited_pages.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71668e6c",
   "metadata": {},
   "source": [
    "## Editors counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1f6e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/editors/aggregate/en.wikipedia/{x}/{y}/{z}/daily/20100101/20220101\"\n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "l3 = ['1..4-edits','5..24-edits','25..99-edits','100..-edits']\n",
    "\n",
    "combined = [(f,s,a) for f in l1 for s in l2 for a in l3]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1],z=j[2]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','activity-level','granularity'])\n",
    "    if i == 0:\n",
    "        edit_counts = d\n",
    "    else: \n",
    "        edit_counts = pd.concat([edit_counts,d])\n",
    "edit_counts.reset_index().to_csv(\"./data/editors.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdd533",
   "metadata": {},
   "source": [
    "## Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f79cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/edits/aggregate/en.wikipedia/{x}/{y}/daily/20150101/20220101\"\n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "\n",
    "combined = [(f,s) for f in l1 for s in l2]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','granularity'])\n",
    "    if i == 0:\n",
    "        edits = d\n",
    "    else: \n",
    "        edits = pd.concat([edits,d])\n",
    "edits.reset_index().to_csv(\"./data/edits.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cf005",
   "metadata": {},
   "source": [
    "## new users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58fba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/registered-users/new/en.wikipedia/daily/20100101/20220101\"\n",
    "    \n",
    "new_users = pd.json_normalize(pd.read_json(base_html)['items'], \n",
    "              record_path =['results'], \n",
    "              meta =['project','granularity'])\n",
    "        \n",
    "new_users.reset_index().to_csv(\"./data/registered_users.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f7f36",
   "metadata": {},
   "source": [
    "## Edit byte difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde94b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/bytes-difference/net/aggregate/en.wikipedia/{x}/{y}/daily/20150101/20220101\"\n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "\n",
    "combined = [(f,s) for f in l1 for s in l2]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','granularity'])\n",
    "    if i == 0:\n",
    "        diff_sum = d\n",
    "    else: \n",
    "        diff_sum = pd.concat([diff_sum,d])\n",
    "diff_sum.reset_index().to_csv(\"./data/bytes_difference_net.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e273124",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_html = \"https://wikimedia.org/api/rest_v1/metrics/bytes-difference/absolute/aggregate/en.wikipedia/{x}/{y}/daily/20150101/20220101\"\n",
    "l1 = ['anonymous','group-bot','name-bot','user']\n",
    "l2 = ['content','non-content']\n",
    "\n",
    "combined = [(f,s) for f in l1 for s in l2]\n",
    "\n",
    "for i,j in enumerate(combined):\n",
    "    d = pd.json_normalize(pd.read_json(base_html.format(x=j[0],y=j[1]))['items'], \n",
    "                                        record_path =['results'], \n",
    "                                        meta =['project','editor-type','page-type','granularity'])\n",
    "    if i == 0:\n",
    "        diff_abs = d\n",
    "    else: \n",
    "        diff_abs = pd.concat([diff_abs,d])\n",
    "        \n",
    "diff_abs.reset_index().to_csv(\"./data/bytes_difference_absolute.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
